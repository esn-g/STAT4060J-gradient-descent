---
title: "hw5"
output: pdf_document
date: "2024-11-27"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Homework 5: SVM

By Erik Simert Nordgren November 2024.

## 1. Vanilla SVM

In this first part we implement the SVM from the lecture slides, with preprocessing given. We find that the algorithm can find a perfect classifier resulting in 0 training and testing loss. From this we conclude the data is linearly separable. The pre-processing algorithm includes the 'sample' function, which samples randomly to create the testing and training sets. This division of the data impacts the convergence of the algorithm, where sometimes, the SVM converges to a perfect classifier instantly. See the code below. For this seed, the convergence was not instant.
```{r}
library(data.table) # allows us to use function fread,
# which quickly reads data from csv files 

# load data
load_digits <- function(subset=NULL, normalize=TRUE) {
  
#Load digits and labels from digits.csv.

#Args:
#subset: A subset of digit from 0 to 9 to return.
#If not specified, all digits will be returned.
#normalize: Whether to normalize data values to between 0 and 1.

#Returns:
#digits: Digits data matrix of the subset specified.
#The shape is (n, p), where
#n is the number of examples,
#p is the dimension of features.
#labels: Labels of the digits in an (n, ) array.
#Each of label[i] is the label for data[i, :]

# load digits.csv, adopted from sklearn.

df <- fread("digits.csv") 
df <- as.matrix(df)

## only keep the numbers we want.
if (length(subset)>0) {
  
  c <- dim(df)[2]
  l_col <- df[,c]
  index = NULL
  
  for (i in 1:length(subset)){
    
    number = subset[i]
    index = c(index,which(l_col == number))
  }
  sort(index)
  df = df[index,]
}

# convert to arrays.
digits = df[,-1]
labels = df[,c]

# Normalize digit values to 0 and 1.
if (normalize == TRUE) {
  digits = digits - min(digits)
digits = digits/max(digits)}


# Change the labels to 0 and 1.
for (i in 1:length(subset)) {
  labels[labels == subset[i]] = i-1
}

return(list(digits, labels))

}

split_samples <- function(digits,labels) {

# Split the data into a training set (70%) and a testing set (30%).

num_samples <- dim(digits)[1]
num_training <- round(num_samples*0.7)
indices = sample(1:num_samples, size = num_samples)
training_idx <- indices[1:num_training]
testing_idx <- indices[-(1:num_training)]

return (list(digits[training_idx,], labels[training_idx],
        digits[testing_idx,], labels[testing_idx]))
}


#====================================
# Load digits and labels.
result = load_digits(subset=c(1, 7), normalize=TRUE)
digits = result[[1]]
labels = result[[2]]

result = split_samples(digits,labels)
training_digits = result[[1]]
training_labels = matrix(result[[2]])
testing_digits = result[[3]]
testing_labels = matrix(result[[4]])

# print dimensions
length(training_digits)
length(testing_digits)

# Train a model and display training accuracy.
##### Put your work here

my_SVM <- function(X_train, Y_train, X_test, Y_test, lambda = 0.01, num_iterations = 100, learning_rate = 0.1) {
  n <- dim(X_train)[1]
  p <- dim(X_train)[2] + 1
  X_train1 <- cbind(rep(1, n), X_train)
  Y_train <- 2 * Y_train - 1
  beta <- matrix(rep(0, p), nrow = p)
  ntest <- nrow(X_test)
  X_test1  <- cbind(rep(1, ntest), X_test) 
  Y_test <- 2 * Y_test - 1
  acc_train <- rep(0, num_iterations)
  acc_test <- rep(0, num_iterations)
  
  for(it in 1:num_iterations) {
    s <- X_train1 %*% beta
    db <- s * Y_train < 1
    dbeta <- matrix(rep(1, n), nrow = 1) %*%((matrix(db*Y_train, n, p)*X_train1))/n; 
    beta <- beta + learning_rate * t(dbeta)
    beta[2:p] <- beta[2:p] - lambda * beta[2:p]
    acc_train[it] <- mean(sign(s * Y_train))
    acc_test[it] <- mean(sign(X_test1 %*% beta * Y_test)) }
  model <- list(beta = beta, acc_train = acc_train, acc_test = acc_test)
  return(model)
}
n_iter = 100
model <- my_SVM(training_digits, training_labels,testing_digits,testing_labels, num_iterations = n_iter)
model
# plot
linx <- c(1:n_iter)
plot(linx, model$acc_train, main = "SVM training and testing accuracy", xlab = "X", ylab = "Y")
lines(linx, model$acc_train, col = "blue" )
lines(linx, model$acc_test,  col = "red" )
grid()
#https://www.rdocumentation.org/packages/graphics/versions/3.6.2/topics/legend
legend("topright",
  legend = c("Training","Testing"),
   inset = c(0.05,0.15) , # change location of legend
  col = c("blue","red"),
  lty = 1,
  lwd = 2,
  # xpd = TRUE, # allow legend outside plot
  #cex = 0.8 # size of legend text
)
```


## Exploring Slackness and Kernel

### Slackness

The slackness parameter C adjusts the tolerance to how many samples we can miss- classify. For small C, we can not achieve perfect accuracy classifier, since the margin is very large, and by definition must include some missclassified samples. We study the testing and training accuracy for different C, and see that for small values of C, the testing and or training accuracy never reaches 1. Testing accuracy seems to suffer the most, but generally, our classifier seems pretty adequate, even for low C (high slack). We plot the margin over C and see that it is decreasing as expected. Interestingly, increasing the number of iterations seems to decrease the margin. This could be because C makes the problem more difficult to minimize. Increasing iterations, we find the margin for $C=0.01$ to converge towards 1.2.
```{r}
my_SVMC <- function(X_train, Y_train, X_test, Y_test, lambda = 0.01, C = 1, num_iterations = 100, learning_rate = 0.1) {
  n <- dim(X_train)[1]
  p <- dim(X_train)[2] + 1
  X_train1 <- cbind(rep(1, n), X_train) 
  Y_train <- 2 * Y_train - 1  
  
  beta <- matrix(rep(0, p), nrow = p) 
  ntest <- nrow(X_test)
  X_test1 <- cbind(rep(1, ntest), X_test) 
  Y_test <- 2 * Y_test - 1
  
  acc_train <- rep(0, num_iterations)
  acc_test <- rep(0, num_iterations)

  for (it in 1:num_iterations) {
    s <- X_train1 %*% beta
    db <- s * Y_train < 1
    # slackness parameter C introduced into the gradient
    # https://www.cs.cornell.edu/courses/cs4780/2018fa/lectures/lecturenote09.html
    dbeta <- C * matrix(rep(1, n), nrow = 1) %*% ((matrix(db * Y_train, n, p) * X_train1)) / n
    beta <- beta + learning_rate * t(dbeta)  # Update weights
    beta[2:p] <- beta[2:p] - learning_rate * lambda * beta[2:p]
    acc_train[it] <- mean(sign(s) == Y_train)
    acc_test[it] <- mean(sign(X_test1 %*% beta) == Y_test)
  }
  
  beta_norm <- sqrt(sum(beta[2:p]^2))
  margin <- 1 / beta_norm
  
  model <- list(beta = beta, acc_train = acc_train, acc_test = acc_test, margin = margin)
  return(model)
}

# Investigate margin size for different levels of slackness C. We call the function for different C values and plot the margin
C_values <- c(0.01, 0.1, 1, 10, 100)
margins <- c(length(C_values))
accuracy_train_list <- list()
accuracy_test_list <- list()
n_iter = 1000

k=0 
# this plot for different C values was implemented in collaboration with chatGPT
for (i in (C_values)){
  k= k+1
  model <- my_SVMC(X_train = training_digits,Y_train = training_labels,X_test = testing_digits,Y_test = testing_labels,lambda = 0.01, C = i, num_iterations = n_iter, learning_rate = 0.1
  )
  margins[k] <- model$margin
  accuracy_train_list[[k]] <- model$acc_train
  accuracy_test_list[[k]] <- model$acc_test
}
# Plot accuracies for each C
par(mfrow = c(1, 1))  # Single plot
colors <- c("red", "blue", "green", "purple", "orange")  # Different colors for each C

plot(
  1:n_iter, accuracy_train_list[[1]], 
  type = "l", col = colors[1], ylim = c(0.7, 1), 
  xlab = "Iteration", ylab = "Accuracy", 
  main = "Training and Testing Accuracy for Different C"
)

lines(1:n_iter, accuracy_test_list[[1]], col = colors[1], lty = 2)

for (i in 2:length(C_values)) {
  lines(1:n_iter, accuracy_train_list[[i]], col = colors[i])  # Training accuracy
  lines(1:n_iter, accuracy_test_list[[i]], col = colors[i], lty = 2)  # Testing accuracy
  grid()
}

legend(
  "bottomright", legend = paste("C =", C_values), 
  col = colors, lty = 1, title = "Training", bty = "n"
)
legend(
  "topright", legend = paste("C =", C_values), 
  col = colors, lty = 2, title = "Testing", bty = "n"
)


# Plot the margins against C values
plot(
  C_values, margins, 
  type = "b",  
  log = "x",   # Logarithmic scale
  xlab = "Slackness, C", 
  ylab = "Margin",
  main = "Effect of Slackness Parameter C on Margin",
  pch = 16,    # Solid points
  col = "blue"
)
grid()

```


### Kernel
Below we explore adding kernel functions to the SVM. Guided by 'A Practical Guide to Support Vector Classification' by Chih-Wei Hsu, Chih-Chung Chang, and Chih-Jen Lin, we implement a Radial Basis Function (RBF) as the kernel. The radial basis function is popular and very good for starting out with kernel based SVM. It is able to account for very strange decision boundaries, i.e. nonlinear ones. However, since we have a linearely separable dataset. We do not expect to see any improvement to our 'perfect' classifier. We instead explore different values of $\gamma$ to see what our RBF can do. Indeed, we reach convergence to $100\%$ accuracy very fast. In fact, the convergence of training and testing accuracy is very fast for $\gamma=\{1, 0.5, 0.1\}$, much faster than with the linear SVM created from the start. This may be simply due to that our beta has many more parameters to define the function separating our classifiers. The betas now describe a more sophisticated and higher dimensional structure than a hyperplane, as it does in the linear SVM. For $\gamma = 0.01$ the SVM with RBF kernel is unable to find any perfect accuracy classifier.

The $\gamma$ parameter is related to the variance. When $\gamma$ is low, the kernel function considers a large region of the samples. This leads to a very general classifier, and much of the reason for using the kernel is lost. Large gamma makes the decision boundary a very local one, which can lead to overfitting. For this problem, a too low $\gamma$ would be $\gamma = 0.01$, but it varies for the specific problem, and hyper parameter tuning is required for each problem.

It would be interesting to understand how the margin varied with the choice of gamma, and this could be an area of further research.

```{r}
# kernel
# https://www.csie.ntu.edu.tw/~cjlin/papers/guide/guide.pdf
# https://www.youtube.com/watch?v=Q0ExqOphnW0
rbf_kernel <- function(X1, X2, gamma) {
    n1 <- nrow(X1)
    n2 <- nrow(X2)
    # Initialize K, the kernel matrix, which gets dimensions from the rows in the matricies considered. 
    K <- matrix(0, nrow = n1, ncol = n2)
    for (i in 1:n1) {
      for (j in 1:n2) {
        K[i, j] <- exp(-gamma*sum((X1[i, ] - X2[j, ])^2))
      }
    }
    return(K)
}

my_SVMK <- function(X_train, Y_train, X_test, Y_test, lambda = 0.01, C = 1, num_iterations = 100, learning_rate = 0.1, gamma = 1) {
  n <- dim(X_train)[1]
  X_train1 <- cbind(rep(1, n), X_train) 
  Y_train <- 2 * Y_train - 1  
  
  # Kernel for training data
  K_train <- rbf_kernel(X_train1,X_train1, gamma)
  p <- dim(K_train)[2] # + 1
  
  beta <- matrix(rep(0, p), nrow = p) 
  ntest <- nrow(X_test)
  X_test1 <- cbind(rep(1, ntest), X_test) 
  Y_test <- 2 * Y_test - 1
  
  # kernel for testing data, needs dimensionality of X_train, since beta gets the dimension of x_train rows
  K_test <- rbf_kernel(X_test1, X_train1, gamma)
  #print(dim(K_train))
  #print(dim(K_test))
  #print(dim(beta))
  acc_train <- rep(0, num_iterations)
  acc_test <- rep(0, num_iterations)

  for (it in 1:num_iterations) {
    # kernel matrix becomes the predictors
    s <- K_train %*% beta
    db <- s * Y_train < 1
    # slackness parameter C introduced into the gradient
    # https://www.cs.cornell.edu/courses/cs4780/2018fa/lectures/lecturenote09.html
    # dbeta <- C * matrix(rep(1, n), nrow = 1) %*% ((matrix(db * Y_train, n, p) * X_train1)) / n
    dbeta <- matrix(rep(1, n), nrow = 1) %*% ((matrix(db * Y_train, n, p) * K_train)) / n
    beta <- beta + learning_rate * t(dbeta)  # Update weights
    beta[2:p] <- beta[2:p] - learning_rate * lambda * beta[2:p]
    acc_train[it] <- mean(sign(s) == Y_train)
    acc_test[it] <- mean(sign(K_test %*% beta) == Y_test)
  }
  
  beta_norm <- sqrt(sum(beta[2:p]^2))
  margin <- 1 / beta_norm
  
  model <- list(beta = beta, acc_train = acc_train, acc_test = acc_test, margin = margin)
  return(model)
}

  model <- my_SVMK(X_train = training_digits,Y_train = training_labels,X_test = testing_digits,Y_test = testing_labels,lambda = 0.01, num_iterations = n_iter, learning_rate = 0.1)
  # model$acc_test
  # model$acc_train
  
  # Code adapted from the ChatGPT generated code in the Slackness plot.
gamma_values <- c(0.01, 0.05, 0.1, 0.5, 1)
accuracy_train_list <- list()
accuracy_test_list <- list()
k = 0
n_iter2 <-100
for (gamma in gamma_values) {
  k = k + 1
  model <- my_SVMK(X_train = training_digits,Y_train = training_labels,X_test = testing_digits,Y_test = testing_labels,lambda = 0.01,num_iterations =n_iter2,learning_rate = 0.1,gamma = gamma)

  accuracy_train_list[[k]] <- model$acc_train
  accuracy_test_list[[k]] <- model$acc_test
}

# Plot training and testing accuracies for each gamma
par(mfrow = c(1, 1))  # Single plot
colors <- rainbow(length(gamma_values))
plot(
  1:n_iter2, accuracy_train_list[[1]],
  type = "l", col = colors[1], ylim = c(0.5, 1),
  xlab = "Iteration", ylab = "Accuracy",
  main = "Training and Testing Accuracy for Different Gamma"
)

lines(1:n_iter2, accuracy_test_list[[1]], col = colors[1], lty = 2)

for (i in 2:length(gamma_values)) {
  lines(1:n_iter2, accuracy_train_list[[i]], col = colors[i])  # Training accuracy
  lines(1:n_iter2, accuracy_test_list[[i]], col = colors[i], lty = 2)  # Testing accuracy
  grid()
}

legend(
  "bottomright", legend = paste("Gamma =", gamma_values),
  col = colors, lty = 1, title = "Training", bty = "n",title.font = 2
)
legend(
  "topright", legend = paste("Gamma =", gamma_values),
  col = colors, lty = 2, title = "Testing", bty = "n",title.font = 2
)
  

```



